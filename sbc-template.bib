@inproceedings{rothe2018positioning,
  title={Positioning of Subtitles in Cinematic Virtual Reality.},
  author={Rothe, Sylvia and Tran, Kim and Hussmann, Heinrich},
  booktitle={ICAT-EGVE},
  pages={1--8},
  year={2018}
}

@article{brown2018exploring,
  title={Exploring Subtitle Behaviour for 360 Video.[White Paper]},
  author={Brown, A and Turner, J and Patterson, J and Schmitz, A and Armstrong, M and Glancy, M},
  journal={British Broadcasting Company},
  year={2018}
}

@INPROCEEDINGS{mendes_2020,
  author={P. {Mendes} and A. {Guedes} and D. {Moraes} and R. {Azevedo} and S. {Colcher}},
  booktitle={2020 IEEE International Conference on Multimedia   Expo Workshops (ICMEW)}, 
  title={An Authoring Model for Interactive 360 Videos}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ICMEW46912.2020.9105958}}
  
@article{agullo2019making,
  title={Making interaction with virtual reality accessible: rendering and guiding methods for subtitles},
  author={Agull{\'o}, Bel{\'e}n and Montagud, Mario and Fraile, Isaac},
  journal={AI EDAM},
  volume={33},
  number={4},
  pages={416--428},
  year={2019},
  publisher={Cambridge University Press}
} 

@article{hayati2011effect,
  title={The effect of films with and without subtitles on listening comprehension of EFL learners},
  author={Hayati, Abdolmajid and Mohmedi, Firooz},
  journal={British Journal of Educational Technology},
  volume={42},
  number={1},
  pages={181--192},
  year={2011},
  publisher={Wiley Online Library}
}

@inproceedings{skupin2016tile,
  title={Tile based HEVC video for head mounted displays},
  author={Skupin, Robert and Sanchez, Yago and Hellge, Cornelius and Schierl, Thomas},
  booktitle={2016 IEEE International Symposium on Multimedia (ISM)},
  pages={399--400},
  year={2016},
  organization={IEEE}
}
%%%%%%%%%%%%%%%%%%%%%%% FROM SLR %%%%%%%%%%%%%%%%%%%%%%%%5 
  
@article{montagud_culture_2020,
	title = {Culture 4 all: accessibility-enabled cultural experiences through immersive {VR360} content},
	issn = {1617-4917},
	shorttitle = {Culture 4 all},
	url = {https://doi.org/10.1007/s00779-019-01357-3},
	doi = {10.1007/s00779-019-01357-3},
	abstract = {The use of virtual reality (VR) technologies and, in particular, of VR360 content can provide great benefits in the society. This particularly applies to the culture and heritage sectors, where venues, goods and events can be digitalised to provide hyper-realistic and engaging experiences, anywhere and anytime. Despite significant advances in the field of VR, there is still an unexplored aspect which is crucial for every service and experience where user interaction is expected: accessibility. This article firstly reviews the needs, challenges and limitations for making VR360 experiences accessible. Based on these facts, an end-to-end platform to efficiently integrate accessibility services within VR360 content is presented. The platform encompasses all steps from media authoring to media consumption, but special attention is given to the accessibility-enabled VR360 player, as it is the end user interaction interface. The presentation modes for the supported access services (like subtitling, audio description and sign language), interaction modalities and personalisation features supported by the player are described. To conclude, the availability of newly created and adapted culture-related accessible VR360 content is explored, as a proof of the potential of the contributions work in this sector.},
	language = {en},
	urldate = {2020-09-11},
	journal = {Personal and Ubiquitous Computing},
	author = {Montagud, Mario and Orero, Pilar and Matamala, Anna},
	month = jan,
	year = {2020},
	keywords = {useful},
	file = {Montagud et al_2020_Culture 4 all.pdf:C\:\\Users\\paulo\\Zotero\\storage\\QWHBLKB7\\Montagud et al_2020_Culture 4 all.pdf:application/pdf}
}

@inproceedings{li_impacts_2018,
	title = {The {Impacts} of {Subtitles} on 360-{Degree} {Video} {Journalism} {Watching}},
	doi = {10.1109/ICIME.2018.00035},
	abstract = {This research focused on the subtitles of 360-degree video journalism to explore the impact of subtitles presentation on video viewing behavior. 27 participants were asked to watch the 360-degree video journalism by using the Head Mount Display (HMD). The content which the participants watched was recorded, and the participants were required to complete a memory test related to the video journalism content after watching. By analyzing the content of the video and combining with the results of the memory test, the results showed that participants' subtitle viewing behavior had certain commonality, that was, participants paid more attention to the position with subtitles, and the subtitle viewing behavior was affected by the screen content; moreover, the duration time of the subtitles was positively related to video journalism content memory.},
	booktitle = {2018 {International} {Joint} {Conference} on {Information}, {Media} and {Engineering} ({ICIME})},
	author = {Li, Ke and Yang, Di and Ji, Suhe and Liu, Liqun},
	month = dec,
	year = {2018},
	keywords = {Virtual reality, human factors, Streaming media, Visualization, 360-degree video journalism watching, 360-degree video journalism, subtitle, subtitle behavior, virtual reality, behavioural sciences computing, head mount display, helmet mounted displays, HMD, journalism, Magnetic heads, Media, Memory management, Resists, subtitle viewing behavior, subtitles presentation, video cameras, video journalism content memory, video recording, video viewing behavior, useful},
	pages = {130--134},
	file = {Li et al_2018_The Impacts of Subtitles on 360-Degree Video Journalism Watching.pdf:C\:\\Users\\paulo\\Zotero\\storage\\MF57NTD3\\Li et al_2018_The Impacts of Subtitles on 360-Degree Video Journalism Watching.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\paulo\\Zotero\\storage\\QERAUKX3\\8609513.html:text/html}
}

@inproceedings{meira_video_2016,
	title = {Video annotation for immersive journalism using masking techniques},
	doi = {10.1109/EPCGI.2016.7851189},
	abstract = {This paper proposes an interactive annotation technique for 360° videos that allows the use of traditional video editing techniques to add content to immersive videos. Using the case study of immersive journalism the main objective is to diminish the entry barrier for annotating 360° video pieces, by providing a different annotation paradigm and a set of tools for annotation. The spread of virtual reality systems and immersive content has been growing substantially due to technological progress and cost reductions in equipment and software. From all the technologies employed in virtual reality systems, 360° video is one that currently presents unique conditions to be widely used by various industries - especially for communication purposes. From the various areas that can benefit from the usage of virtual reality systems, the communication field is one that requires innovation in the way that narratives are built, especially in virtual reality systems. In the case of immersive journalism, 360° video technology is currently one of the most used mediums by several media outlets. This kind of news content, whose innovative role should be highlighted, is still being studied in the field of journalism, needing a clearly defined set of rules and good practises. In order to improve the introduction of virtual elements in the 360° videos this paper proposes a set of annotation paradigms for 1) Media information display and 2) Narrative and attention focusing. In this paper we present a list of possible techniques that solve the problem of immersive annotation, as well as a description of a prototype that was developed to test these concepts. The prototype implements an annotation technique based on masked videos and the extension of standard subtitle file formats. Finally a fast-track user study was developed to evaluate the acceptance of the visualisation techniques and to refine the set of tools.},
	booktitle = {2016 23rd {Portuguese} {Meeting} on {Computer} {Graphics} and {Interaction} ({EPCGI})},
	author = {Meira, João and Marques, João and Jacob, João and Nóbrega, Rui and Rodrigues, Rui and Coelho, António and de Sousa, A. Augusto},
	month = nov,
	year = {2016},
	keywords = {Virtual reality, virtual reality, Virtual Reality, image annotation, Streaming media, video signal processing, Visualization, Media, 360° Video, Annotation, Computer Graphics, immersive journalism, Immersive Journalism, Interaction, interactive annotation technique, masking techniques, Pipelines, Prototypes, social sciences computing, Software, subtitle file formats, video annotation, video editing techniques, virtual reality systems, useful},
	pages = {1--7},
	file = {Meira et al_2016_Video annotation for immersive journalism using masking techniques.pdf:C\:\\Users\\paulo\\Zotero\\storage\\2PGPFA5C\\Meira et al_2016_Video annotation for immersive journalism using masking techniques.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\paulo\\Zotero\\storage\\LZQFIWZY\\7851189.html:text/html}
}

@inproceedings{chen_film_2017,
	title = {Film language analysis in society news-{A} case study of {The} {New} {York} {Times}},
	doi = {10.23919/PNC.2017.8203523},
	abstract = {As the virtual reality is famous with its immersion, it makes journalism a promising application of 360 videos. However, the key factors of VR videos, such as interaction, framing, stitching, etc., differ greatly from that of what they used to be, it appears that storytelling is getting much more difficult than before. To deal with the problem, we analyze the New York Times 360 videos, a pioneer in immersive journalism, and want to find the film language in the 360 video. After analyzing, we concluded there were 6 key factors: surrounding, subtitles, perspective, shot size, guidance, and angle of the 360 video. In addition, we did the user study to confirm the result of ours. Finally, we provide a guideline for 360 video director to shot a 360 videos with immersion and understanding.},
	booktitle = {2017 {Pacific} {Neighborhood} {Consortium} {Annual} {Conference} and {Joint} {Meetings} ({PNC})},
	author = {Chen, Te-Yuan and Liu, Hsuan-Chi and Hsu, Chia-Yu},
	month = nov,
	year = {2017},
	keywords = {Virtual reality, virtual reality, video signal processing, journalism, immersive journalism, 360 video director, Cameras, film language, film language analysis, IEEE Society news, Information science, Lenses, Libraries, linguistics, publishing, shot a 360 videos, society news, society news analysis, The New York Times, Videos, VR videos, useful},
	pages = {63--68},
	file = {Chen et al_2017_Film language analysis in society news-A case study of The New York Times.pdf:C\:\\Users\\paulo\\Zotero\\storage\\M6RYVZ7Y\\Chen et al_2017_Film language analysis in society news-A case study of The New York Times.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\paulo\\Zotero\\storage\\MJI7UJ6C\\8203523.html:text/html}
}

@inproceedings{brown_subtitles_2017,
	address = {New York, NY, USA},
	series = {{TVX} '17 {Adjunct}},
	title = {Subtitles in 360-degree {Video}},
	isbn = {978-1-4503-5023-5},
	url = {https://doi.org/10.1145/3084289.3089915},
	doi = {10.1145/3084289.3089915},
	abstract = {Currently there exists no agreed-upon user experience guidelines regarding subtitling (closed captions) in immersive 360-degree video experiences. It is not clear how subtitles might be acceptably displayed within this context, namely to support the balance between comprehension, freedom to look around the scene, and immersion. This work-in-progress describes four subtitle behaviours that we have designed and implemented in order to perform user-testing. We describe our rationale for each behaviour and discuss our initial hypotheses surrounding a full empirical investigation.},
	urldate = {2020-09-10},
	booktitle = {Adjunct {Publication} of the 2017 {ACM} {International} {Conference} on {Interactive} {Experiences} for {TV} and {Online} {Video}},
	publisher = {Association for Computing Machinery},
	author = {Brown, Andy and Turner, Jayson and Patterson, Jake and Schmitz, Anastasia and Armstrong, Mike and Glancy, Maxine},
	month = jun,
	year = {2017},
	keywords = {user experience, 360-degree video, accessibility, hci., subtitles, vr, useful},
	pages = {3--8},
	file = {Brown et al_2017_Subtitles in 360-degree Video.pdf:C\:\\Users\\paulo\\Zotero\\storage\\UL9PGAUM\\Brown et al_2017_Subtitles in 360-degree Video.pdf:application/pdf}
}

@inproceedings{hughes_disruptive_2019,
	address = {New York, NY, USA},
	series = {{TVX} '19},
	title = {Disruptive {Approaches} for {Subtitling} in {Immersive} {Environments}},
	isbn = {978-1-4503-6017-3},
	url = {https://doi.org/10.1145/3317697.3325123},
	doi = {10.1145/3317697.3325123},
	abstract = {The Immersive Accessibility Project (ImAc) explores how accessibility services can be integrated with 360o video as well as new methods for enabling universal access to immersive content. ImAc is focused on inclusivity and addresses the needs of all users, including those with sensory or learning disabilities, of all ages and considers language and user preferences. The project focuses on moving away from the constraints of existing technologies and explores new methods for creating a personal experience for each consumer. It is not good enough to simply retrofit subtitles into immersive content: this paper attempts to disrupt the industry with new and often controversial methods. This paper provides an overview of the ImAc project and proposes guiding methods for subtitling in immersive environments. We discuss the current state-of-the-art for subtitling in immersive environments and the rendering of subtitles in the user interface within the ImAc project. We then discuss new experimental rendering modes that have been implemented including a responsive subtitle approach, which dynamically re-blocks subtitles to fit the available space and explore alternative rendering techniques where the subtitles are attached to the scene.},
	urldate = {2020-09-10},
	booktitle = {Proceedings of the 2019 {ACM} {International} {Conference} on {Interactive} {Experiences} for {TV} and {Online} {Video}},
	publisher = {Association for Computing Machinery},
	author = {Hughes, Chris and Montagud Climent, Mario and tho Pesch, Peter},
	month = jun,
	year = {2019},
	keywords = {360 video, Accessibility, Immersive video, Subtitling, useful},
	pages = {216--229},
	file = {Hughes et al_2019_Disruptive Approaches for Subtitling in Immersive Environments.pdf:C\:\\Users\\paulo\\Zotero\\storage\\2TNPG8CN\\Hughes et al_2019_Disruptive Approaches for Subtitling in Immersive Environments.pdf:application/pdf}
}

@book{matamala2010listening,
  title={Listening to subtitles: subtitles for the deaf and hard of hearing},
  author={Matamala, Anna and Orero, Pilar},
  year={2010},
  publisher={Peter Lang}
}

@inproceedings{brown_dynamic_2015,
	address = {New York, NY, USA},
	series = {{TVX} '15},
	title = {Dynamic {Subtitles}: {The} {User} {Experience}},
	isbn = {978-1-4503-3526-3},
	shorttitle = {Dynamic {Subtitles}},
	url = {https://doi.org/10.1145/2745197.2745204},
	doi = {10.1145/2745197.2745204},
	abstract = {Subtitles (closed captions) on television are typically placed at the bottom-centre of the screen. However, placing subtitles in varying positions, according to the underlying video content (`dynamic subtitles'), has the potential to make the overall viewing experience less disjointed and more immersive. This paper describes the testing of such subtitles with hearing-impaired users, and a new analysis of previously collected eye-tracking data. The qualitative data demonstrates that dynamic subtitles can lead to an improved User Experience, although not for all types of subtitle user. The eye-tracking data was analysed to compare the gaze patterns of subtitle users with a baseline of those for people viewing without subtitles. It was found that gaze patterns of people watching dynamic subtitles were closer to the baseline than those of people watching with traditional subtitles. Finally, some of the factors that need to be considered when authoring dynamic subtitles are discussed.},
	urldate = {2020-09-10},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Interactive} {Experiences} for {TV} and {Online} {Video}},
	publisher = {Association for Computing Machinery},
	author = {Brown, Andy and Jones, Rhia and Crabb, Mike and Sandford, James and Brooks, Matthew and Armstrong, Mike and Jay, Caroline},
	month = jun,
	year = {2015},
	keywords = {user experience, accessibility, subtitles, attention approximation, eye-tracking, hci, tv, useful},
	pages = {103--112},
	file = {Brown et al_2015_Dynamic Subtitles.pdf:C\:\\Users\\paulo\\Zotero\\storage\\3VBD8E9U\\Brown et al_2015_Dynamic Subtitles.pdf:application/pdf}
}

@inproceedings{rothe_dynamic_2018,
	address = {New York, NY, USA},
	series = {{TVX} '18},
	title = {Dynamic {Subtitles} in {Cinematic} {Virtual} {Reality}},
	isbn = {978-1-4503-5115-7},
	url = {https://doi.org/10.1145/3210825.3213556},
	doi = {10.1145/3210825.3213556},
	abstract = {Cinematic Virtual Reality has been increasing in popularity in recent years. Watching 360° movies with a Head Mounted Display, the viewer can freely choose the direction of view, and thus the visible section of the movie. Therefore, a new approach for the placements of subtitles is needed. There are three main issues which have to be considered: the position of the subtitles, the speaker identification and the influence for the VR experience. In our study we compared a static method, where the subtitles are placed at the bottom of the field of view, with dynamic subtitles 1, where the position of the subtitles depends on the scene and is close to the speaking person. This work-in-progress describes first results of the study which point out that dynamic subtitles can lead to a higher score of presence, less sickness and lower workload.},
	urldate = {2020-09-10},
	booktitle = {Proceedings of the 2018 {ACM} {International} {Conference} on {Interactive} {Experiences} for {TV} and {Online} {Video}},
	publisher = {Association for Computing Machinery},
	author = {Rothe, Sylvia and Tran, Kim and Hussmann, Heinrich},
	month = jun,
	year = {2018},
	keywords = {360-degree video, subtitles, cinematic virtual reality, dynamic subtitles, presence, screen-referenced subtitles, sickness, speaker identification, static subtitles, task workload, useful},
	pages = {209--214},
	file = {Rothe et al_2018_Dynamic Subtitles in Cinematic Virtual Reality.pdf:C\:\\Users\\paulo\\Zotero\\storage\\TTCZZ8W8\\Rothe et al_2018_Dynamic Subtitles in Cinematic Virtual Reality.pdf:application/pdf}
}

@inproceedings{matos_dynamic_2018,
	address = {New York, NY, USA},
	series = {{Web3D} '18},
	title = {Dynamic annotations on an interactive web-based 360º video player},
	isbn = {978-1-4503-5800-2},
	url = {https://doi.org/10.1145/3208806.3208818},
	doi = {10.1145/3208806.3208818},
	abstract = {The use of 360° videos has been increasing steadily in the 2010s, as content creators and users search for more immersive experiences. The freedom to choose where to look at during the video may hinder the overall experience instead of enhancing it, as there is no guarantee that the user will focus on relevant sections of the scene. Visual annotations superimposed on the video, such as text boxes or arrow icons, can help guide the user through the narrative of the video while maintaining freedom of movement. This paper presents a web-based immersive visualizer for 360° videos that contain dynamic media annotations, rendered in real-time. A set of annotations was created with the purpose of providing information or guiding the user to points of interest. The visualizer can be used with a computer, using a keyboard and mouse or HTC Vive, and in mobile devices with Cardboard VR headsets, to experience the video in virtual reality, which is made possible with the WebVR API. The visualizer was evaluated through usability tests, to analyze the impact of different annotation techniques on the users' experience. The obtained results demonstrate that annotations can assist in guiding the user during the video, and a careful design is imperative so that they are not intrusive and distracting for the viewers.},
	urldate = {2020-09-10},
	booktitle = {Proceedings of the 23rd {International} {ACM} {Conference} on {3D} {Web} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Matos, Teresa and Nóbrega, Rui and Rodrigues, Rui and Pinheiro, Marisa},
	month = jun,
	year = {2018},
	keywords = {360° videos, annotations, virtual reality, webVR, useful},
	pages = {1--4},
	file = {Matos et al_2018_Dynamic annotations on an interactive web-based 360&#xb0\; video player.pdf:C\:\\Users\\paulo\\Zotero\\storage\\F37Y8LC4\\Matos et al_2018_Dynamic annotations on an interactive web-based 360&#xb0\; video player.pdf:application/pdf}
}

@inproceedings{nguyen_depth_2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {Depth {Conflict} {Reduction} for {Stereo} {VR} {Video} {Interfaces}},
	isbn = {978-1-4503-5620-6},
	url = {https://doi.org/10.1145/3173574.3173638},
	doi = {10.1145/3173574.3173638},
	abstract = {Applications for viewing and editing 360° video often render user interface (UI) elements on top of the video. For stereoscopic video, in which the perceived depth varies over the image, the perceived depth of the video can conflict with that of the UI elements, creating discomfort and making it hard to shift focus. To address this problem, we explore two new techniques that adjust the UI rendering based on the video content. The first technique dynamically adjusts the perceived depth of the UI to avoid depth conflict, and the second blurs the video in a halo around the UI. We conduct a user study to assess the effectiveness of these techniques in two stereoscopic VR video tasks: video watching with subtitles, and video search.},
	urldate = {2020-09-10},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Cuong and DiVerdi, Stephen and Hertzmann, Aaron and Liu, Feng},
	month = apr,
	year = {2018},
	keywords = {virtual reality, subtitles, 360, stereoscopic, video interface, useful},
	pages = {1--9},
	file = {Nguyen et al_2018_Depth Conflict Reduction for Stereo VR Video Interfaces.pdf:C\:\\Users\\paulo\\Zotero\\storage\\NBUKFS6L\\Nguyen et al_2018_Depth Conflict Reduction for Stereo VR Video Interfaces.pdf:application/pdf}
}

%%%%%%%%%%%%%% evaluation methods %%%%%%%%%%%%%%%%%%%

@incollection{nasa_hart1988development,
  title={Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research},
  author={Hart, Sandra G and Staveland, Lowell E},
  booktitle={Advances in psychology},
  volume={52},
  pages={139--183},
  year={1988},
  publisher={Elsevier}
}

@article{sickness_kennedy1993simulator,
  title={Simulator sickness questionnaire: An enhanced method for quantifying simulator sickness},
  author={Kennedy, Robert S and Lane, Norman E and Berbaum, Kevin S and Lilienthal, Michael G},
  journal={The international journal of aviation psychology},
  volume={3},
  number={3},
  pages={203--220},
  year={1993},
  publisher={Taylor \& Francis}
}

@article{presence_witmer1998measuring,
  title={Measuring presence in virtual environments: A presence questionnaire},
  author={Witmer, Bob G and Singer, Michael J},
  journal={Presence},
  volume={7},
  number={3},
  pages={225--240},
  year={1998},
  publisher={MIT Press}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%5 nausea %%%%%%%%%%%%%%%%%%%%%

@article{laviola2000discussion,
  title={A discussion of cybersickness in virtual environments},
  author={LaViola Jr, Joseph J},
  journal={ACM Sigchi Bulletin},
  volume={32},
  number={1},
  pages={47--56},
  year={2000},
  publisher={ACM New York, NY, USA}
}

@article{sharples2008virtual,
  title={Virtual reality induced symptoms and effects (VRISE): Comparison of head mounted display (HMD), desktop and projection display systems},
  author={Sharples, Sarah and Cobb, Sue and Moody, Amanda and Wilson, John R},
  journal={Displays},
  volume={29},
  number={2},
  pages={58--69},
  year={2008},
  publisher={Elsevier}
}